from selenium import webdriverfrom selenium.webdriver.chrome.service import Servicefrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom webdriver_manager.chrome import ChromeDriverManagerfrom selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedExceptionimport timeimport jsonfrom urllib.parse import urlencodeimport spacyfrom spacy.language import Languagefrom spacy.tokens.span import Spanfrom sentimental_onix import pipelinefrom bertopic import BERTopicimport reimport dashfrom dash import dcc, htmlfrom dash.dependencies import Input, Outputimport plotly.graph_objs as goimport plotly.express as pximport numpy as npfrom collections import Counterfrom sentence_transformers import SentenceTransformerLOAD_FROM_METADATA_FILE = TrueLOAD_FROM_TRANSCRIPT_FILE = TrueCOMBINED_OUTPUT = Falseclass SpeechScraper:    def __init__(self):        self.base_url = 'https://rollcall.com/factbase/trump/search/'        self.params = {            "q": "",            "f": "",            "media": "",            "type": "speech",            "sort": "desc",            "page": 0        }        self.driver = None        self.speech_metadata = []        self.speeches = []        self.metadata_filename = 'trump_rally_speech_metadata.json'        self.transcript_filename = 'trump_rally_speeches.json'    def load_speech_metadata_from_file(self):        # Load the JSON data from file        with open(self.metadata_filename, 'r') as file:            self.speech_metadata = json.load(file)    def load_speech_transcripts_from_file(self):        # Load the JSON data from file        with open(self.transcript_filename, 'r') as file:            self.speeches = json.load(file)    def setup_driver(self):        chrome_options = Options()        # chrome_options.add_argument("--headless")  # Run in headless mode        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36")        service = Service(ChromeDriverManager().install())        return webdriver.Chrome(service=service, options=chrome_options)    def close_popup_ad_type1(self):        try:            # Wait for ad wrapper id to show            ad_wrapper = WebDriverWait(self.driver, 5).until(                EC.visibility_of_element_located((By.ID, "adWrapper"))            )            # Click close icon            close_icon = ad_wrapper.find_element(By.ID, "closeIcon")            close_icon.click()            # Wait for ad close            WebDriverWait(self.driver, 5).until(                EC.invisibility_of_element_located((By.ID, "adWrapper"))            )            return True        except (TimeoutException, NoSuchElementException, ElementClickInterceptedException) as e:            print(f"No pop-up ad type 1 found or unable to close it: {e}")            return False    def close_popup_ad_type2(self):        try:            # Wait for the ad close button to be visible            close_button = WebDriverWait(self.driver, 5).until(                EC.element_to_be_clickable((By.CSS_SELECTOR, "a.bx-close.bx-close-link.bx-close-inside"))            )                        # Click close icon            close_button.click()                        # Wait for ad close            WebDriverWait(self.driver, 5).until(                EC.invisibility_of_element_located((By.CLASS_NAME, "bx-wrap"))            )                        return True        except (TimeoutException, NoSuchElementException, ElementClickInterceptedException) as e:            print(f"No pop-up ad type 2 found or unable to close it: {e}")            return False    def close_popup_ads(self):        ad1_closed = self.close_popup_ad_type1()        ad2_closed = self.close_popup_ad_type2()        return ad1_closed or ad2_closed    def is_rally_speech(self, title):        rally_keywords = ['rally', 'political rally']        return any(keyword in title.lower() for keyword in rally_keywords)    def is_speaker(self, speaker, title):        return speaker.lower() in title.lower()    def scroll_and_extract(self, target_count=80):        data = []        last_item_count = 0        no_new_items_count = 0        max_attempts = 10        total_items_checked = 0        while len(data) < target_count and no_new_items_count < max_attempts:            # Scroll down            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")            time.sleep(2)  # Wait for the page to load            # Check for and close pop-up ads            self.close_popup_ads()            # Extract data            items = self.driver.find_elements(By.CSS_SELECTOR, "div.mb-4.border.rounded-md.shadow-md.p-6.hover\\:shadow-2xl")            for item in items[total_items_checked:]:                try:                    title = item.find_element(By.CSS_SELECTOR, "h4.font-semibold.mb-4").text                    if self.is_rally_speech(title):                        date = item.find_element(By.CSS_SELECTOR, "div.flex h4.text-xs").text                        link = item.find_element(By.CSS_SELECTOR, "a[href^='https://rollcall.com/factbase/trump/transcript/']").get_attribute("href")                        data.append({"date": date, "title": title, "link": link})                        if len(data) >= target_count:                            break                except (NoSuchElementException, StaleElementReferenceException) as e:                    print(f"Error extracting item: {e}")                total_items_checked += 1            if len(data) == last_item_count:                no_new_items_count += 1            else:                no_new_items_count = 0            last_item_count = len(data)            print(f"Current number of extracted rally speech_metadata: {last_item_count}")        return data    def extract_speech_metadata(self, target_count=80):        self.setup_driver()        print("Starting to scrape speech_metadata")        full_url = f"{self.base_url}?{urlencode(self.params)}"        print(f"Navigating to URL: {full_url}")        self.driver.get(full_url)        try:            # Wait for the initial content to load            WebDriverWait(self.driver, 10).until(                EC.presence_of_element_located((By.CSS_SELECTOR, "div.mb-4.border.rounded-md.shadow-md.p-6.hover\\:shadow-2xl"))            )        except TimeoutException:            print("Timeout waiting for initial items to load. Exiting.")            self.driver.quit()            return []        print("Initial content loaded. Starting to scroll and extract data.")        self.speech_metadata = self.scroll_and_extract(target_count)        print(f"Extracted {len(self.speech_metadata)} rally speech_metadata")        self.driver.quit()        return self.speech_metadata    def extract_speech_transcripts(self, speaker):        self.setup_driver()        data = []        for metadata in self.speech_metadata:            speech_url = metadata['link']            print(f"Navigating to URL: {speech_url}")            self.driver.get(speech_url)            # Extract data #mb-4 border rounded-md shadow-md p-6 hover:shadow-2xl            items = self.driver.find_elements(By.CSS_SELECTOR, "div.mb-4.border.rounded-md.shadow-md.p-6.hover\\:shadow-2xl")            transcript_data = ''            for item in items:                try:                    title = item.find_element(By.CSS_SELECTOR, "div.w-3\\/12 h2.font-semibold.text-lg").text                    if self.is_speaker(speaker, title):                        item_transcript = item.find_element(By.CSS_SELECTOR, "div.w-full.sm\\:w-9\\/12 div.leading-normal.text-gray-700.italic.relative span.text-lg.sm\\:text-md").text                        transcript_data += item_transcript                except (NoSuchElementException, StaleElementReferenceException) as e:                    print(f"Error extracting item: {e}")            data.append({"title": metadata['title'], "date": metadata['date'],  "link": metadata['link'], "transcript": transcript_data})        self.speeches = data        return self.speeches    def save_transcripts_to_file(self):        with open(self.transcript_filename, "w") as f:            json.dump(self.speeches, f, indent=2)        print(f"Saved {len(self.speeches)} rally speeches to {self.transcript_filename}")    def save_metadata_to_file(self):        with open(self.metadata_filename, "w") as f:            json.dump(self.speech_metadata, f, indent=2)        print(f"Saved {len(self.speech_metadata)} rally speech_metadata to {self.metadata_filename}")    def get_speech_metadata(self, count=None):        if count:            return self.speech_metadata[:count]        else:            return self.speech_metadata    def get_speech_transcripts(self, count=None):        if count:            return self.speeches[:count]        else:            return self.speeches@Language.component("remove_possessive")def remove_possessive(doc):    doc.ents = _remove_possessive_generator(doc)    return docdef _remove_possessive_generator(doc):    # Yields non-possessive versions of the given document's entities.    for ent in doc.ents:        if ent.text.endswith("'s") or ent.text.endswith("'"):            yield Span(doc, ent.start, ent.end-1, label=ent.label_)        else:            yield ent#TODO: Account for mispellings as a result of bad transcriptionclass SpeechAnalyzer:    def __init__(self, transcripts, combine_output=False):        # Setup spacy nlp object        self.nlp = spacy.load('en_core_web_sm')        self.nlp.add_pipe("remove_possessive", last=True)        self.nlp.add_pipe("sentencizer")        self.nlp.add_pipe("sentimental_onix", after="sentencizer")        self.nlp.max_length = 2000000        # Set class attributes        self.combined_output = combine_output        self.transcripts = []                # Clean transcripts and assign to class transcripts object        for transcript in transcripts:            self.transcripts.append({                "title": transcript['title'],                "transcript": self.clean_transcript(transcript['transcript'])            })        # Setup dashboard        self.app = dash.Dash(__name__)        self.setup_layout()                # Setup topic model using BERTopic        self.sentence_model = SentenceTransformer("all-MiniLM-L6-v2")        self.topic_model = BERTopic(embedding_model=self.sentence_model)        # self.topic_model.fit([transcript['transcript'] for transcript in self.transcripts])    def run(self):        self.app.run_server(debug=True)    ############# DATA PROCESSING FUNCTIONS #############    def get_combined_transcripts(self):        return ' '.join(transcript['transcript'] for transcript in self.transcripts)    def clean_transcript(self, transcript):        # Remove content within square brackets        cleaned_transcript = re.sub(r'\[.*?\]', '', transcript)        # Remove backslashes        cleaned_transcript = cleaned_transcript.replace('\\', '')        # Normalize quotation marks        cleaned_transcript = re.sub(r'["""]', '"', cleaned_transcript)        # Remove extra whitespace        cleaned_transcript = re.sub(r'\s+', ' ', cleaned_transcript).strip()        return cleaned_transcript    def get_word_count(self, text):        # Process text in chunks if it's too long        chunk_size = 2000000        word_freq = Counter()        for i in range(0, len(text), chunk_size):            chunk = text[i:i+chunk_size]            doc = self.nlp(chunk)            tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]            word_freq.update(tokens)        return sorted(word_freq.items(), key=lambda x: (-x[1], x[0]))    def get_sentence_sentiment(self, text, get_sentiment_value=False):        sentiment_map = {            "positive": 1,            "neutral": 0,            "negative": -1        }        chunk_size = 2000000        sentence_sentiments = []        for i in range(0, len(text), chunk_size):            chunk = text[i:i+chunk_size]            doc = self.nlp(chunk)            chunk_sentiments = [                (sent.text, sent._.sentiment if get_sentiment_value else sentiment_map.get(sent._.sentiment.lower(), 0))                for item in self.nlp.pipe(                    [str(x) for x in list(doc.sents)]                )                for sent in item.sents            ]            sentence_sentiments.extend(chunk_sentiments)        # return sorted(sentence_sentiments, key=lambda x: (-sentiment_map.get(x[1].lower(), 0), x[0]))        return sentence_sentiments    def identify_entities(self, text):        doc = self.nlp(text)        entities = [(ent.text, ent.label_) for ent in doc.ents]                # Merge entities based on full names        merged_entities = {}        for entity, label in entities:            # Check if the entity is a person            if label == "PERSON":                # Use the full name of the entity                full_name = entity                                # Check if the entity's name is a substring of an existing entity                merged = False                for existing_entity in merged_entities:                    if full_name in existing_entity or existing_entity in full_name:                        merged_entities[existing_entity].append((entity, label))                        merged = True                        break                                # If the entity was not merged, add it as a new entity                if not merged:                    merged_entities[full_name] = [(entity, label)]        # return merged_entities    def merge_entities(self, entity_sentiments):        merged_entities = {}        for entity, sentiments in entity_sentiments.items():            # Check if the entity is a person            if entity.label_ == "PERSON":                # Use the full name of the entity                full_name = entity.text                                # Check if the entity's name is a substring of an existing entity                merged = False                for existing_entity in merged_entities:                    if full_name in existing_entity or existing_entity in full_name:                        merged_entities[existing_entity].extend(sentiments)                        merged = True                        break                                # If the entity was not merged, add it as a new entity                if not merged:                    merged_entities[full_name] = sentiments            else:                merged_entities[entity.text] = sentiments        # Calculate average sentiment for each merged entity        for entity, sentiments in merged_entities.items():            merged_entities[entity] = sum(sentiments) / len(sentiments)        return merged_entities    def get_entity_sentiment(self, text, window_size=2):        doc = self.nlp(text)                sentence_sentiments = self.get_sentence_sentiment(text)        sentiment_map = {sent: sentiment for sent, sentiment in sentence_sentiments}                excluded_labels = {"MONEY", "QUANTITY", "CARDINAL", "PERCENT"}        entity_sentiments = {}        for ent in doc.ents:            # Filter out stop words, numbers, exluded labels            if not ent[0].is_stop and not ent[0].like_num and ent.label_ not in excluded_labels:                if ent not in entity_sentiments:                    entity_sentiments[ent] = []                # Find sentences within the specified window size                for sent in doc.sents:                    if abs(ent.sent.start - sent.start) <= window_size:                        sentiment = sentiment_map.get(sent.text, 0)                        entity_sentiments[ent].append(sentiment)                # Merge entities based on full names        merged_entity_sentiments = self.merge_entities(entity_sentiments)                return merged_entity_sentiments    def get_topics(self, text):        # Process the specific text        doc = self.nlp(text)                # Infer topics for the specific text        topics, probs = self.topic_model.fit_transform([str(x) for x in list(doc.sents)])                # Retrieve topic information        topic_list = self.topic_model.get_topic_info()                for topic in topic_list.itertuples():            if topic.Topic != -1:  # Exclude the -1 topic which is used for outliers                print(f"Topic {topic.Topic}: {', '.join(word for word, _ in self.topic_model.get_topic(topic.Topic))}")        # return topic_info        # doc = self.nlp_topic(text)        # # topics, probs = self.topic_model.transform([str(x) for x in list(doc.sents)])        # topics, probs = self.topic_model.transform([doc.text])        # # fig = self.topic_model.visualize_topics()        # # fig.show()        # return topics, probs    ############# DATA OUTPUT FUNCTIONS #############    def get_word_count_data(self, transcript_index=None):        if transcript_index is None:            if self.combined_output:                combined_text = self.get_combined_transcripts()                return dict(self.get_word_count(combined_text))            else:                return [dict(self.get_word_count(transcript['transcript'])) for transcript in self.transcripts]        else:            assert 0 <= transcript_index < len(self.transcripts)            return dict(self.get_word_count(self.transcripts[transcript_index]['transcript']))        def get_sentence_level_sentiment(self, transcript_index=None):        if transcript_index is None:            if self.combined_output:                combined_text = self.get_combined_transcripts()                return dict(self.get_word_count(combined_text))            else:                return [dict(self.get_word_count(transcript['transcript'])) for transcript in self.transcripts]        else:            assert 0 <= transcript_index < len(self.transcripts)            print(self.get_sentence_sentiment(self.transcripts[transcript_index]['transcript']))    def get_entity_data(self, transcript_index=None):        if transcript_index is None:            if self.combined_output:                combined_text = ' '.join(t['transcript'] for t in self.transcripts)                entities = self.identify_entities(combined_text)                entity_sentiments = self.get_entity_sentiment(combined_text)                return {'entities': entities, 'entity_sentiments': entity_sentiments}            else:                return [{                    'title': t['title'],                    'entities': self.identify_entities(t['transcript']),                    'entity_sentiments': self.get_entity_sentiment(t['transcript'])                } for t in self.transcripts]        else:            assert 0 <= transcript_index < len(self.transcripts)            transcript = self.transcripts[transcript_index]            return {                'title': transcript['title'],                'entities': self.identify_entities(transcript['transcript']),                'entity_sentiments': self.get_entity_sentiment(transcript['transcript'])            }    def get_topic_data(self, transcript_index=None):        transcript = self.transcripts[transcript_index]['transcript']        return self.get_topics(transcript)    ############# TERMINAL OUTPUT FUNCTIONS #############    def print_sentences(self, transcript_index=None):        def process_and_print_sentences(self, text):            # Process text in chunks if it's too long            chunk_size = 2000000  # Adjust this value based on your system's capabilities            for i in range(0, len(text), chunk_size):                chunk = text[i:i+chunk_size]                doc = self.nlp(chunk)                for sentence in doc.sents:                    print(sentence)        if transcript_index is None:            text = self.get_combined_transcripts() if self.combined_output else ''            for i, transcript in enumerate(self.transcripts):                if not self.combined_output:                    print(f"\nTranscript {i + 1}: {transcript['title']}")                text += transcript['transcript'] if self.combined_output else transcript['transcript']                if not self.combined_output:                    process_and_print_sentences(transcript['transcript'])            if self.combined_output:                process_and_print_sentences(text)        else:            assert 0 <= transcript_index < len(self.transcripts)            transcript = self.transcripts[transcript_index]            print(f"\nTranscript: {transcript['title']}")            process_and_print_sentences(transcript['transcript'])    def print_word_count(self, transcript_index=None):        if transcript_index is None:            if self.combined_output:                combined_text = self.get_combined_transcripts()                print("\nCombined Word Count:")                for word, count in self.get_word_count(combined_text):                    print(f"{word}: {count}")            else:                for i, transcript in enumerate(self.transcripts):                    print(f"\nTranscript {i + 1}: {transcript['title']}")                    for word, count in self.get_word_count(transcript['transcript']):                        print(f"{word}: {count}")        else:            assert 0 <= transcript_index < len(self.transcripts)            transcript = self.transcripts[transcript_index]            print(f"\nTranscript: {transcript['title']}")            for word, count in self.get_word_count(transcript['transcript']):                print(f"{word}: {count}")    def print_sentence_level_sentiment(self, transcript_index=None):        if transcript_index is None:            if self.combined_output:                combined_text = self.get_combined_transcripts()                print("\nCombined Sentence Sentiment:")                for sentence, sentiment in self.get_sentence_sentiment(combined_text):                    print(f"{sentence}: {sentiment}")            else:                for i, transcript in enumerate(self.transcripts):                    print(f"\nTranscript {i + 1}: {transcript['title']}")                    for sentence, sentiment in self.get_sentence_sentiment(transcript['transcript']):                        print(f"{sentence}: {sentiment}")        else:            assert 0 <= transcript_index < len(self.transcripts)            transcript = self.transcripts[transcript_index]            print(f"\nTranscript: {transcript['title']}")            for sentence, sentiment in self.get_sentence_sentiment(transcript['transcript']):                print(f"{sentence}: {sentiment}")    def print_entity_data(self, transcript_index=None):        data = self.get_entity_data(transcript_index)        if isinstance(data, list):            for transcript_data in data:                print(f"\nTranscript: {transcript_data['title']}")                print("\nTop Entities:")                for entity, count in transcript_data['entities'].most_common(10):                    print(f"{entity}: {count}")                print("\nEntity Sentiments:")                for entity, sentiment in transcript_data['entity_sentiments'].items():                    print(f"{entity}: {sentiment}")        else:            print(f"\nTranscript: {data['title'] if 'title' in data else 'Combined'}")            print("\nTop Entities:")            for entity, count in data['entities'].most_common(10):                print(f"{entity}: {count}")            print("\nEntity Sentiments:")            for entity, sentiment in data['entity_sentiments'].items():                print(f"{entity}: {sentiment}")    ############# UI OUTPUT FUNCTIONS #############    def setup_layout(self):        self.app.layout = html.Div([            html.H1("Speech Analyzer"),            dcc.Dropdown(                id='transcript-dropdown',                options=[{'label': t['title'], 'value': i} for i, t in enumerate(self.transcripts)],                value=0            ),            dcc.Tabs([                dcc.Tab(label='Word Count', children=[                    dcc.Loading(                        id="loading-word-count",                        type="circle",                        children=dcc.Graph(id='word-count-graph')                    )                ]),                dcc.Tab(label='3D Word Cloud', children=[                    dcc.Loading(                        id="loading-wordcloud",                        type="circle",                        children=dcc.Graph(id='wordcloud-3d')                    )                ]),                dcc.Tab(label='Sentence Analysis', children=[                    dcc.Loading(                        id="loading-sentiment",                        type="circle",                        children=[                            dcc.Graph(id='sentiment-graph'),                            html.Div(id='colored-sentences', style={'maxHeight': '500px', 'overflow': 'auto'})                        ]                    )                ]),                dcc.Tab(label='Entity Analysis', children=[                    dcc.Loading(                        id="loading-entity",                        type="circle",                        children=[                            dcc.Graph(id='entity-graph'),                            html.Div(id='entity-sentiments', style={'maxHeight': '500px', 'overflow': 'auto'})                        ]                    )                ])            ])        ])        @self.app.callback(            Output('word-count-graph', 'figure'),            Input('transcript-dropdown', 'value')        )        def update_word_count_graph(selected_transcript):            word_counts = self.get_word_count(self.transcripts[selected_transcript]['transcript'])            fig = px.bar(                x=[word for word, count in word_counts],                y=[count for word, count in word_counts],                labels={'x': 'Word', 'y': 'Count'},                title=f"Word Counts in {self.transcripts[selected_transcript]['title']}",                height=600            )            fig.update_layout(xaxis={'categoryorder':'total descending'})            return fig        @self.app.callback(            Output('wordcloud-3d', 'figure'),            Input('transcript-dropdown', 'value')        )        def update_3d_wordcloud(selected_transcript):            return self.generate_3d_wordcloud(num_words=100, transcript_index=selected_transcript)        @self.app.callback(            [Output('sentiment-graph', 'figure'),             Output('colored-sentences', 'children')],            Input('transcript-dropdown', 'value')        )        def update_sentiment_analysis(selected_transcript):            sentiments = self.get_sentence_sentiment(self.transcripts[selected_transcript]['transcript'], True)            sentiment_counts = Counter(sentiment for _, sentiment in sentiments)                        fig = px.pie(                values=list(sentiment_counts.values()),                names=list(sentiment_counts.keys()),                title=f"Sentiment Distribution in {self.transcripts[selected_transcript]['title']}"            )                        color_map = {                'positive': 'green',                'neutral': 'gray',                'negative': 'red'            }                        colored_sentences = [                html.P(                    sentence,                    style={'color': color_map.get(sentiment.lower(), 'black')}                )                for sentence, sentiment in sentiments            ]                        return fig, colored_sentences        @self.app.callback(            [Output('entity-graph', 'figure'),             Output('entity-sentiments', 'children')],            Input('transcript-dropdown', 'value')        )        def update_entity_analysis(selected_transcript):            def get_sentiment_color(sentiment):                if sentiment > 0:                    return 'green'                elif sentiment < 0:                    return 'red'                else:                    return 'gray'            entity_data = self.get_entity_data(selected_transcript)            # Only going to show positive/negative sentiments on bar chart            filtered_entity_counts = {entity: sentiment for entity, sentiment in entity_data['entity_sentiments'].items() if sentiment != 0}                        colors = [get_sentiment_color(sentiment) for sentiment in filtered_entity_counts.values()]                        fig = px.bar(                x=list(filtered_entity_counts.keys()),                y=list(filtered_entity_counts.values()),                labels={'x': 'Entity', 'y': 'Sentiment'},                title=f"Top Entities in {self.transcripts[selected_transcript]['title']}",                height=600,                color=colors,                color_discrete_map="identity"            )            fig.update_layout(xaxis={'categoryorder':'total descending'})            entity_sentiments = [                html.P(                    f"{entity}: {sentiment}",                    style={'color': get_sentiment_color(sentiment)}                )                for entity, sentiment in entity_data['entity_sentiments'].items()            ]                        return fig, entity_sentiments    def generate_3d_wordcloud(self, num_words=100, transcript_index=None):        word_count_data = self.get_word_count_data(transcript_index)                # Sort words by frequency and take top 'num_words'        sorted_words = sorted(word_count_data.items(), key=lambda x: x[1], reverse=True)[:num_words]        words, frequencies = zip(*sorted_words)        x = np.random.rand(num_words)        y = np.random.rand(num_words)        z = np.random.rand(num_words)        norm_frequencies = [(freq - min(frequencies)) / (max(frequencies) - min(frequencies)) for freq in frequencies]        trace = go.Scatter3d(            x=x, y=y, z=z,            text=words,            mode='text+markers',            marker=dict(                size=1,                color='red',                colorscale='Viridis',                opacity=0.5            ),            textfont=dict(                size=[freq/max(frequencies)*70 for freq in frequencies],  # Scale font size                color='red'            )        )        layout = go.Layout(            scene=dict(                xaxis=dict(visible=False),                yaxis=dict(visible=False),                zaxis=dict(visible=False)            ),            margin=dict(r=0, l=0, b=0, t=0)        )        return go.Figure(data=[trace], layout=layout)    def visualize_sentence_sentiment(self, transcript_index=None):        sentiment_colors = {            "positive": "green",            "neutral": "gray",            "negative": "red"        }        sentiment_data = None        if transcript_index is None:            if self.combined_output:                combined_text = self.get_combined_transcripts()                sentiment_data = self.get_sentence_sentiment(combined_text)            else:                sentiment_data = []                for transcript in self.transcripts:                    sentiment_data.extend(self.get_sentence_sentiment(transcript['transcript']))        else:            assert 0 <= transcript_index < len(self.transcripts)            sentiment_data = self.get_sentence_sentiment(self.transcripts[transcript_index]['transcript'])        sentences = [f"{i+1}. {sent}" for i, (sent, _) in enumerate(sentiment_data)]        colors = [sentiment_colors.get(sent.lower(), "black") for _, sent in sentiment_data]        fig = go.Figure(data=[go.Table(            header=dict(values=["Sentences colored by sentiment"]),            cells=dict(values=[sentences],                       fill_color=['white'],                       font=dict(color=[colors]),                       align='left')        )])        fig.update_layout(            title="Sentence Sentiment Visualization",            height=30 * len(sentences) + 50,            width=1000        )        fig.show()def main():    scraper = SpeechScraper()    if LOAD_FROM_TRANSCRIPT_FILE:        scraper.load_speech_transcripts_from_file()    else:        # Get speech metadata        if LOAD_FROM_METADATA_FILE:            scraper.load_speech_metadata_from_file()        else:            speech_metadata = scraper.extract_speech_metadata(80)  # Get most recent 80 rally speeches            scraper.save_metadata_to_file()            print(f"Total rally speech_metadata scraped: {len(speech_metadata)}")        # Get speech transcripts        scraper.extract_speech_transcripts('Trump')        scraper.save_transcripts_to_file()    print('Data loaded... Processing data.')    analyzer = SpeechAnalyzer(scraper.get_speech_transcripts())    # Get word counts    # word_counts = analyzer.print_word_count()    # Generate word clodu    # analyzer.generate_3d_wordcloud()    # Get sentence level sentiment    # analyzer.print_sentence_level_sentiment(1)    # analyzer.visualize_sentence_sentiment(1)    # print(analyzer.get_entity_data(1))    # print(analyzer.get_topic_data(0))    analyzer.run()if __name__ == '__main__':    main()